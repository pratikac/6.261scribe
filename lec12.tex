\documentclass[letterpaper, 11pt, reqno]{amsart}

\usepackage{mathpazo}
\linespread{1.1}
\include{macros}
\usepackage[margin=1.25in]{geometry}

\title{Lecture 12: Learning distributions over permutations}
\date{May 9, 2014}
\author{Pratik Chaudhari}

\begin{document}
\maketitle

In this lecture, we will discuss distributions over permutations and derive bounds for sampling complexity of learning such distributions using optimization techniques from the previous few lectures.

\section{Some background}

Given $n$ objects, let $\Sn$ be the space of all its permutations. Note that $\abs{\Sn} = n!$. Let $\m : \Sn \mapsto [0,1]$ be some distribution over the permutations; it has $n!-1$ degrees of freedom. Our question in this class will ``under what conditions can we learn $\m$''. In particular, we would like a sample complexity that scales as $\bigo(poly(n))$. A number of situations where this problem makes sense are:
\ilist{
    \item
    \emph{People's choice}: Given $n$ different products and each data point is a set of products and the product chosen by the individual, we would like to learn the ordering of all the $n$ items. In other words, this model, known as the ``rational view model'' and proposed by Samuelson in 1937, presumes that an individual has a perfect ordering of the $n$ items and picks the item with the highest score from the subset shown to her. This model is however full of contradictions because choices of individuals vary based on the current setting, e.g., we may prefer Mexican food over Italian food someday and vice-versa.
    %
    \item
    \emph{Discrete choice models}: In this setting, an individual's choice is a distribution over permutations and these choices are reveled through preferences such as ``picking item A over item B''. For example, this is a good model for a sports tournament where team A defeats team B, C defeats D, etc. and we would like to understand the ordering between the teams.
    \ilist{
        \item
        ``Ranked elections'', which are popular in the Commonwealth countries provide another example. In this election, voters, instead of voting for a particular candidiate, provide a list of preferences. Assume every voter $v$ provides an ordering of $n$ candiates, i.e, $\s_v \in \Sn$, so given $\s_1,
        \ldots, \s_m \in \Sn$, find $\s^* \sim \mu$.
        \item
        Imagine if there are only 2 candidates, we can easily find the overall winner of this election as the one who has most votes for position 1. This motivates a simple algorithm to decide between $n$ candidiates --- at each iteration remove the least voted candiate at position 1 and decide the winner after $n-1$ such iterations. It is easy to see that this algorithm however does not work if the electorate is highly polarized and there is a candidate who has 100\% votes for the second position.
        \item See~\cite{diaconis1989generalization} for an empirical analysis using spectral methods for ranked data from elections.
    }
}

\subsection{Arrow's impossibility theorem}
Consider the following criteria, they seem like a good set of conditions that we would like to impose upon an algorithm that decides the ``best winner'' using partial preferences.
\ilist{
    \item \emph{fairness}: the algorithm weights all votes equally
    \item \emph{sanity check}: if a certain learnt distribution $\hat{\mu}$ is such that its corresponding $\s^*(\hat{\mu})$ assigns $i > j$ for some $i$ and $j$, if we revert a few negative data-points, i.e., ones with $j > i$ to positive, i.e., $i > j$, and get a new $\hat{\mu}'$ then the new $\s^*(\hat{\mu}')$ still has $i > j$. %Note that reflexivity, i.e., if $\s^*(\hat{\mu})$ dictates $i > j$ and $j > k$ then it also dictates $i > k$, holds trivially.
}

Arrow's impossibility theorem~\cite{arrow2012social} states that with three or more candidates, no ranked ordering system can convert the ranked preferences of the voters into a community-wide ranking while also maintaining the above criteria. Put other way, the above conditions lead to a dictatorial election procedure, i.e., one which prefers a certain candidate over others irrespective of the data.

See the article \href{http://tech.mit.edu/V123/N8/8voting.8n.html}{Arrow's Theorem Proves No Voting System is Perfect} for a nice discussion.

\section{Multinomial Logit Model}

The above discussion helps to concretize the problem formulation.

\begin{problem}
Given $\mu: \Sn \mapsto [0,1]$, iid samples from $\mu$, say $\s_t$ are drawn as follows: at time $t$, sample $\s_t \in \Sn$ according to $\mu$. We however only get to observe the ordering between two items $i_t, j_t$ that are chosen randomly from $n$ items, i.e., pick a pair $i, j$ from the ordered set $\listn = \{1, 2, \ldots, n\}$ and output $i_t > j_t$ or $i_t < j_t$ depending upon their index in $\s_t$. Our objective is to ``learn $\mu$'' from this data.
\end{problem}

The pairs $i, j$ can be chosen a priori or can be desgined using some scheme. Note that data size is $\bigo(n^2)$ and hence learning $\mu$ directly is not possible (since $\mu$ has $n!-1$ parameters), we need to simplify $\mu$ in some way, which we do as follows.

\subsection{Multinomial Logit Model [Thurstone 1927]}

Each item $i$ has a score
$$S_i = u_i + Z_i$$
associated with it where $u_i$ is a deterministic factor, e.g., its inherent ``goodness'' and $Z_i$ is noise, e.g., a team's performance on a given day. $Z_i$ is assumed to independent from everything else, e.g., $Z_i \sim N(0, \rho_i)$ where $\rho_i$ is some parameter based on $i$. The descending order of $S_i$s provides a permutation of $\listn$. \href{http://www.2700chess.com}{Elo} (FIDE's rating system for chess players) uses this model. Another example is Microsoft's TrueSkill gaming engine which uses the $Z_i \sim N(0, \rho_i)$ model.

We will use a different distribution for $Z_i$ here known as the Gumbel distribution. Thus
$$Z_i \sim \gumbel(0,1)$$
where the density of the Gumbel distribution is given by $f(x) = \exp(-x -\exp(-x))$. Note that we are assuming $Z_i$s to be homogeneous, i.e., $S_i \geq S_j$ iff $u_i \geq u_j$. $u_i$s are the parameters that we would like to learn from $\bigo(n \log n)$ samples of the form $i > j$ or not. Note that this model induces an inherently $O(n^2)$ distribution.

\begin{remark}
If $(u_i - u_j$ is large, we can see that this model performs well. Note
\aeqs{
    \P(S_i > S_j) &= \P(Z_i - Z_j > u_j - u_i)\\
    &= P(N(0,2 \rho^2) > u_j - u_i) \quad \trm{if}\ Z_i \sim N(0, \rho^2) \\
    &= 1 - \Phi(\f{u_j -u_i}{\sqrt{2}\rho})
}
where $\Phi(x) = \P(N(0,1) \leq x)$ is the CDF of a Gaussian. If $u_i >> u_j$, we have
$$
\log \P(S_i > S_j)\ \approx\ \f{(u_i -u_j)^2}{4 \rho^2} - \f{1}{2} \log 4 \pi \rho^2.
$$
Thus, $S_i > S_j$ is very likely.
\end{remark}

\subsection{Motivation for Gumbel distribution}
Let us first note a neat fact of this distribution.
\begin{lemma}
\label{lem:gumbel_mgf}
Difference of two Gumbel random variables is a Logistic random variable, i.e.,
$$
X, Y \sim \gumbel(0,1) \implies W = X-Y \sim \logistic(0,1).
$$
In other words,
$$
\P(W < w) = \f{1}{1+e^{-w}}.
$$
\end{lemma}
\begin{proof}
This fact can be proved using convolution, which is messy. We shall use moment generating functions to prove it instead. Note that
\aeqs{
    \E[\exp(\th W)] &= \E[\exp(\th X)\ \exp(-\th Y)] \\
    &= \E[\exp(\th X)] \E[\exp(-\th Y)] \\
    \implies M_W(\th) &= M_X(\th) M_Y(-\th).
}
where $M_W(\th)$ is the moment generating function of $W$. It is known that
$$
\E[M_X(\th)] = \Gamma(1-\th) \exp(\th).
$$
where $\Gamma(n) = (n-1)!$ if $n$ is an integer.
Thus,
\aeqs{
    \E[\exp(\th W)] &= \Gamma(1-\th) \exp(\th) \Gamma(1+\th) \exp(-\th) \\
    &= \Gamma(1-\th) \Gamma(1+\th) \\
    &= \f{\Gamma(1-\th) \Gamma(1+\th)}{\Gamma(2)}\\
    &= \trm{Beta}(1-\th, 1+\th).
}
where $\trm{Beta}(x,y) = \f{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}$ is the Beta distribution which is also the MGF of $\logistic(0,1)$!
\end{proof}

Using Lem.~\ref{lem:gumbel_mgf}, we can see that
\aeqs{
    \P(S_i > S_j) &= \P(u_i + Z_i > u_j + Z_j) \\
    &= \P(Z_i - Z_j > u_j - u_i)\\
    &= \f{1}{1 + \exp(u_i - u_j)}\\
    \implies \P(i > j) &= \f{\exp(u_i - u_j)}{1 + \exp(u_i - u_j)} = \f{e^{u_i}}{e^{u_i} + e^{u_j}}.
}
The probability of $i$ being chosen over $j$ is thus proportional to exponentiated intrinsic skill of $i$. In a sense, the Gumbel distribution we used above is retro-fitted to achive this, which is what was prevalent in literature.

However, quite surprisingly,~\cite{luce2012individual} proved in 1946 that this form for $\P(i > j)$ is the only distribution where we have ``irrelevance of external choices'', e.g., given two options for desert, a chocolate cake and coffee, if one chooses coffee, upon being presented a third option, say mint-tea, the individual does not change her choice to chocolate cake. Note that this is not true for the example of elections, if people prefer the marginal risk of choosing candidate A over B, upon being presented a third option C, the electorate might be so polarized that they will conservatively vote for B to avoid C winning at any cost.

Let us give a simulation procedure for the Gumbel distribution. Let $w_i = e^{u_i}$. We need to generate $\s$ s.t. $\P(i > j) = \f{w_i}{w_i + w_j}$. Start filling up an array of length $n$ where
$$
\P(i \trm{ is picked for current position} ) = \f{w_i}{\sum_k w_k}.
$$
To see that this scheme indeed produces the desired distribution, we use a stopping time argument. Let $\tau$ be the first time when either item $i$ or $j$ is selected. Observe that conditional upon either $i$ or $j$ being picked, the probability of $i$ being picked, i.e., probability of $i$ being chosen over $j$ is simply,
\aeqs{
\P( i > j) = \f{w_i/\sum_k w_k}{w_i/\sum_k w_k + w_j/\sum_k w_k} = \f{w_i}{w_i + w_j}.
}

\section{Formal setup}

Our formal setup is as follows.
\begin{problem}
\label{prob:formal}
Given $m$ observations $(i_k, j_k)$ where $i_k > j_k$, i.e., item $i_k$ is picked over $j_k$, reconstruct the exponentiated ``skill'' parameters $w_1, \ldots, w_n$.
\end{problem}
To solve this problem, we shall maximize the likelihood function. Consider,
\aeqs{
    \P((i_1, j_1), \ldots, (i_m, j_m)) &= \prod_{k=1}^m \P((i_k, j_k)) \\
    &= \prod_{k=1}^m \f{w_{i_k}}{w_{i_k} + w_{j_k}} \\
    &= \prod_{k=1}^m \f{\exp(u_{i_k} - u_{j_k})}{1+\exp(u_{i_k} - u_{j_k})}.
}
Let $x_k \in \{ 0,1\}^n$ be a vector where $x_k(i_k) = 1$, $x_k(j_k) = -1$ and all the others are zeros. $x_k$ thus has a +1 at the $i_k^{th}$ element and a -1 at the $j_k^{th}$ element if $i_k > j_k$. If we let $u = [u_1, \ldots, u_n]'$ we can see that
$$
u' x_k = u_{i_k} - u_{j_k}.
$$
Simplify the above equation to get
\aeqs{
    \P((i_1, j_1), \ldots, (i_m, j_m)) &= \prod_{k=1}^m \f{\exp(u' x_k)}{1 + \exp(u'x_k)}.
}
If we let $-L(u) = \log \P(u; x_1, \ldots, x_m)$ be the log-likelihood function, we have
$$
-L(u) = \sum_{k=1}^m \sqbrac{u' x_k - \log(1+\exp(u'x_k))}.
$$
The ML estimate of $u$ is then the solution of the following problem
\beq{
    \min_{u \in \reals^n, \sum u_i = 0}\ -\sum_{k=1}^m \sqbrac{u' x_k - \log(1+\exp(u'x_k))}.
    \label{eqn:ML_probem}
}
\begin{remark}
Let us assume that we know some bound $b$ s.t. $\norm{u}_\infty \leq b$. Also, note that since $L(u)$ only depends upon terms of the form $u_{i_k} - u_{j_k}$, it is invariant to shifting, so we can set $\sum u_i = 0$ as another constraint in the problem.

Let us note that Prob.~\ref{prob:formal} is a convex optimization with nice constraints and hence can be solved efficiently.
\end{remark}

\subsection{Sample complexity of learning}
\nocite{negahban2012restricted}
%The analysis in this section follows the arguments in~\cite{negahban2012restricted}.
Let $\hu$ be the solution of Prob.~\ref{prob:formal} and $u^*$ be the actual model. By definition, we have
\aeqs{
L(\hu) &\leq L(u^*) \\
\implies L(\hu) - L(u^*) - \nabla L(u^*)' \D &\leq -\nabla L(u^*)' \D
}
where $\D = \hu - u^*$. The idea is to then bound
$$
\norm{\nabla L(u^*)} \leq \alpha
$$
for some $\alpha$ and get
$$
L(\hu) - L(u^*) - \nabla L(u^*)' \D \leq \norm{\nabla L(u^*)}_2\ \norm{\D}_2.
$$
But note that
$$
L(\hu) - L(u^*) - \nabla L(u^*)' \D \geq \f{1}{2} \D' \nabla^2 L(u^* + \lambda \D)\ \D
$$
where $\lambda \in [0,1]$ and thus $u^* + \lambda \D$ lies on the line joining $u^*$ and $\hu$. (This is an application of the mean value theorem).
If we can find some $\beta$ such that
$$
\f{1}{2} \nabla^2 L(u^* + \lambda \D) \geq \beta,
$$
we will have
\aeqs{
\beta \norm{\D}_2^2 \leq L(\hu) - L(u^*) - &\nabla L(u^*)' \D \leq \alpha \norm{\D}_2 \\
\trm{which}\ \implies \norm{\D}_2 \leq \f{\alpha}{\beta}
}
and we thus get a bound on $\norm{\hu - u^*}$.

\subsubsection{Getting $\a$ and $\b$}
Let us first compute $\b$. Note that since $L(\cdot)$ is a sum, we can consider each individual term separately. Also $u' x_k$ terms in Eqn.~\eqref{eqn:ML_probem} vanish while taking the Hessian. Thus if $z = u^* + \l \D$ as we used in the previous section, we have
\aeqs{
    \nabla^2 L(z) &= \f{\partial^2}{\partial z_i\ \partial z_j}\ \log(1 + \exp(z' x_k)) \\
    &= \f{\partial}{\partial z_j}\ \sum_{k=1}^m \f{1}{1+\exp(z' x_k)} \exp(z' x_k)\ x_{ki} \\
    &= \sum_{k=1}^m x_{ki}\ \cbrac{\f{\exp(z'x_k) x_{kj}}{1+\exp(z' x_k)} - \f{\exp(z' x_k)^2 x_{kj}}{(1+\exp(z' x_k))^2}} \\
    &= \sum_{k=1}^m \f{x_{ki} x_{kj}\ \exp(z' x_k)}{1+\exp(z' x_k)} \sqbrac{1 - \f{\exp(z' x_k)}{1+\exp(z' x_k)}} \\
    &= \sum_{k=1}^m \f{x_{ki} x_{kj}\ \exp(z' x_k)}{(1+\exp(z' x_k))^2}.
}
Finally,
\aeqs{
    \D' \nabla^2 L(z) \D = \sum_{k} \sum_{i,j} \f{\D_i \D_j x_{ki} x_{kj} \exp(z' x_k)}{(1+\exp(z' x_k))^2}.
}
Now note that $z = u^* + \l \D$ and we have assumed $\norm{u^*}_\infty \leq b$ and of course $\norm{\D}_\infty \leq b$ (since $\D = \hu - u^*$). Thus
\aeqs{
\norm{z}_\infty &\leq 2 b \qquad \l \in [0,1] \\
&= \abs{z' x_k} \leq 2b.
}
Let $\tilde{b} = \exp(2b)$ and see that
\aeqs{
    \D' \nabla^2 L(z) \D &\geq \f{\tilde{b}}{(1+\tilde{b})^2} \sum_k \sqbrac{\sum_{i,j}\ \D_i \D_j x_{ki} x_{kj}} \\
    & = \f{\tilde{b}}{(1+\tilde{b})^2}\ \sqbrac{\sum_k (\D' x_k)^2}.
}
It can now be shown using matrix concentration inequalities that
\begin{claim}
For iid samples and $m > 12\ n \log n$, w.h.p.
$$
\f{1}{m} \sum_{k=1}^m \rbrac{\D' x_k}^2 \geq \f{1}{3n} \norm{\D}^2_2.
$$
\end{claim}
\begin{claim}
$$\norm{\nabla L(u^*)}_2 \leq 2 \sqrt{\f{\log n}{m}} = \a.$$
\end{claim}
Using the above two claims, we finally have
$$
\norm{\D}_2 \leq \f{\tilde{b}}{(1+\tilde{b})^2}\ \sqrt{\f{n^2 \log n}{m}}.
$$
\begin{remark}
Note that the $\tilde{b}$ terms in the above bound should not really be there, i.e., the error of our algorithm should not depend upon how well separated different candidates are in terms of their skills. Arguably, larger the value of $b$, the easier it is to discover the winner.

\pc{write about the intuition Devavrat gave about the $\f{n^2 \log }{m}$ term here.}{}
\end{remark}


\section{Rank Centrality}
The material in this section follows~\cite{rank_centrality}.

{
\linespread{1.5}
\bibliography{lec12}
\bibliographystyle{apa}
}

\end{document}